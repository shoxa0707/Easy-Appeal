{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]\n",
    "\n",
    "! pip install ipywidgets\n",
    "! jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dzqD2WDFOIN-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:27:00 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "2023-01-28 22:27:00.884118: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-28 22:27:01.434339: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-28 22:27:01.434385: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-28 22:27:01.434389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[NeMo W 2023-01-28 22:27:01 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-01-28 22:27:01 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-01-28 22:27:02 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/torch/jit/annotations.py:309: UserWarning: TorchScript will treat type annotations of Tensor dtype-specific subtypes as if they are normal Tensors. dtype constraints are not enforced in compilation either.\n",
      "      warnings.warn(\"TorchScript will treat type annotations of Tensor \"\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from nemo.collections import nlp as nemo_nlp\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "import os\n",
    "import wget \n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daYw_Xll2ZR9"
   },
   "source": [
    "# Task Description\n",
    "**Sentiment Analysis** is the task of detecting the sentiment in text. We model this problem as a simple form of a text classification problem. For example `Gollum's performance is incredible!` has a positive sentiment while `It's neither as romantic nor as thrilling as it should be.` has a negative sentiment.\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnuziSwJ1yEB"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "In this tutorial we going to use [The Stanford Sentiment Treebank (SST-2)](https://nlp.stanford.edu/sentiment/index.html) corpus for sentiment analysis. This version of the dataset contains a collection of sentences with binary labels of positive and negative. It is a standard benchmark for sentence classification and is part of the GLUE Benchmark: https://gluebenchmark.com/tasks. Please download and unzip the SST-2 dataset from GLUE. It should contain three files of train.tsv, dev.tsv, and test.tsv which can be used for training, validation, and test respectively.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzcZ3nb_-SVT"
   },
   "source": [
    "# NeMo Text Classification Data Format\n",
    "\n",
    "[TextClassificationModel](https://github.com/NVIDIA/NeMo/blob/stable/nemo/collections/nlp/models/text_classification/text_classification_model.py) in NeMo supports text classification problems such as sentiment analysis or domain/intent detection for dialogue systems, as long as the data follows the format specified below. \n",
    "\n",
    "TextClassificationModel requires the data to be stored in TAB separated files (.tsv) with two columns of sentence and label. Each line of the data file contains text sequences, where words are separated with spaces and label separated with [TAB], i.e.: \n",
    "\n",
    "```\n",
    "[WORD][SPACE][WORD][SPACE][WORD][TAB][LABEL]\n",
    "```\n",
    "\n",
    "For example:\n",
    "```\n",
    "hide new secretions from the parental units[TAB]0\n",
    "\n",
    "that loves its characters and communicates something rather beautiful about human nature[TAB]1\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "If your dataset is stored in another format, you need to convert it to this format to use the TextClassificationModel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SL58EWkd2ZVb"
   },
   "source": [
    "## Preprocess the Data\n",
    "\n",
    "First, you need to download the zipped file of the SST-2 dataset from the GLUE Benchmark website: https://gluebenchmark.com/tasks, and put it in the current folder. Then the following script would extract it into the data path specified by `DATA_DIR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n8HZrDmr12_-"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"Data\"\n",
    "WORK_DIR = \"Work\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8Ty5_S7Ye8h"
   },
   "source": [
    "Now, the data folder should contain the following files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8vsyh3JZH26"
   },
   "source": [
    "\n",
    "\n",
    "* train.tsv\n",
    "* dev.tsv\n",
    "* test.tsv\n",
    "\n",
    "\n",
    "The format of `train.tsv` and `dev.tsv` is close to NeMo's format except to have an extra header line at the beginning of the files. We would remove these extra lines. But `test.tsv` has different format and labels are missing for this part of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daludzzL2Jba"
   },
   "source": [
    "# Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_whKCxfTMo6Y"
   },
   "source": [
    "Now, let's take a closer look at the model's configuration and learn to train the model from scratch and finetune the pretrained model.\n",
    "\n",
    "Our text classification model uses a pretrained [BERT](https://arxiv.org/pdf/1810.04805.pdf) model (or other BERT-like models) followed by a classification layer on the output of the first token ([CLS]).\n",
    "\n",
    "The model is defined in a config file which declares multiple important sections. The most important ones are:\n",
    "- **model**: All arguments that are related to the Model - language model, tokenizer, head classifier, optimizer, schedulers, and datasets/data loaders.\n",
    "\n",
    "- **trainer**: Any argument to be passed to PyTorch Lightning including number of epochs, number of GPUs, precision level, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T1gA8PsJ13MJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file already exists\n",
      "2-jamoa uchun/Work/configs/text_classification_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# download the model's configuration file \n",
    "MODEL_CONFIG = \"text_classification_config.yaml\"\n",
    "CONFIG_DIR = WORK_DIR + '/configs/'\n",
    "\n",
    "os.makedirs(CONFIG_DIR, exist_ok=True)\n",
    "if not os.path.exists(CONFIG_DIR + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/text_classification/conf/' + MODEL_CONFIG, CONFIG_DIR)\n",
    "    print('Config file downloaded!')\n",
    "else:\n",
    "    print ('config file already exists')\n",
    "config_path = f'{WORK_DIR}/configs/{MODEL_CONFIG}'\n",
    "print(config_path)\n",
    "config = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCgWzNBkaQLZ"
   },
   "source": [
    "# Model Training From Scratch\n",
    "## Setting up data within the config\n",
    "\n",
    "We first need to set the num_classes in the config file which specifies the number of classes in the dataset. For SST-2, we have just two classes (0-positive and 1-negative). So we set the num_classes to 2. The model supports more than 2 classes too.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jFSMiWtlkaC5"
   },
   "outputs": [],
   "source": [
    "config.model.dataset.num_classes=9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCkc7QGikqPh"
   },
   "source": [
    "\n",
    "Among other things, the config file contains dictionaries called dataset, train_ds and validation_ds. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n",
    "\n",
    "Notice that some config lines, including `model.dataset.classes_num`, have `???` as their value, this means that values for these fields are required to be to be specified by the user. We need to specify and set the `model.train_ds.file_name`, `model.validation_ds.file_name`, and `model.test_ds.file_name` in the config file to the paths of the train, validation, and test files if they exist. We may do it by updating the config file or by setting them from the command line. \n",
    "\n",
    "Let's now set the train and validation paths in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LQHCJN-ZaoLp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataloader's config: \n",
      "\n",
      "file_path: 2-jamoa uchun/Data/train.txt\n",
      "batch_size: 8\n",
      "shuffle: true\n",
      "num_samples: -1\n",
      "num_workers: 3\n",
      "drop_last: false\n",
      "pin_memory: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config.model.train_ds.file_path = os.path.join(DATA_DIR, 'train.txt')\n",
    "config.model.validation_ds.file_path = os.path.join(DATA_DIR, 'test.txt')\n",
    "# Name of the .nemo file where trained model will be saved.\n",
    "config.save_to = 'trained-model.nemo'\n",
    "config.export_to = 'trained-model.onnx'\n",
    "config.model.train_ds.batch_size = 8\n",
    "\n",
    "print(\"Train dataloader's config: \\n\")\n",
    "# OmegaConf.to_yaml() is used to create a proper format for printing the train dataloader's config\n",
    "# You may change other params like batch size or the number of samples to be considered (-1 means all the samples)\n",
    "print(OmegaConf.to_yaml(config.model.train_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB96-3sTc3yk"
   },
   "source": [
    "## Building the PyTorch Lightning Trainer\n",
    "\n",
    "NeMo models are primarily PyTorch Lightning (PT) modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem.\n",
    "\n",
    "Let's first instantiate a PT Trainer object by using the trainer section of the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1tG4FzZ4Ui60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer config - \n",
      "\n",
      "devices: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 100\n",
      "max_steps: -1\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "precision: 32\n",
      "accelerator: gpu\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "resume_from_checkpoint: null\n",
      "num_sanity_val_steps: 0\n",
      "enable_checkpointing: false\n",
      "logger: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainer config - \\n\")\n",
    "# OmegaConf.to_yaml() is used to create a proper format for printing the trainer config\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMVVs0INi5zj"
   },
   "source": [
    "First you need to create a PT trainer with the params stored in the trainer's config. You may set the number of steps for training with max_steps or number of epochs with max_epochs in the trainer's config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "knF6QeQQdMrH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "# lets modify some trainer configs\n",
    "# checks if we have GPU available and uses it\n",
    "config.trainer.accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "config.trainer.devices = 1\n",
    "\n",
    "\n",
    "# for mixed precision training, uncomment the lines below (precision should be set to 16 and amp_level to O1):\n",
    "# config.trainer.precision = 16\n",
    "# config.trainer.amp_level = O1\n",
    "\n",
    "# disable distributed training when using Colab to prevent the errors\n",
    "config.trainer.strategy = None\n",
    "\n",
    "# setup max number of steps to reduce training time for demonstration purposes of this tutorial\n",
    "# Training stops when max_step or max_epochs is reached (earliest)\n",
    "config.trainer.max_epochs = 10\n",
    "\n",
    "# instantiates a PT Trainer object by using the trainer section of the config\n",
    "trainer = pl.Trainer(**config.trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IlEMdVxdr6p"
   },
   "source": [
    "## Setting up the NeMo Experiment¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Kl5IdnV3O8y"
   },
   "source": [
    "NeMo has an experiment manager that handles the logging and saving checkpoints for us, so let's setup it. We need the PT trainer and the exp_manager config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8uztqGAmdrYt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_dir: null\n",
      "name: TextClassification\n",
      "create_tensorboard_logger: true\n",
      "create_checkpoint_callback: true\n",
      "\n",
      "[NeMo I 2023-01-28 22:27:15 exp_manager:343] Experiments will be logged at /home/airi/Desktop/jupyter/til model/text classification/nemo_experiments/TextClassification/2023-01-28_22-27-15\n",
      "[NeMo I 2023-01-28 22:27:15 exp_manager:718] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:27:15 exp_manager:988] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/airi/Desktop/jupyter/til model/text classification/nemo_experiments/TextClassification/2023-01-28_22-27-15\n"
     ]
    }
   ],
   "source": [
    "# The experiment manager of a trainer object can not be set twice. We repeat the trainer creation code again here to prevent getting error when this cell is executed more than once. \n",
    "trainer = pl.Trainer(**config.trainer)\n",
    "\n",
    "# exp_dir specifies the path to store the the checkpoints and also the logs, it's default is \"./nemo_experiments\"\n",
    "# You may set it by uncommentig the following line\n",
    "# config.exp_manager.exp_dir = 'LOG_CHECKPOINT_DIR'\n",
    "\n",
    "# OmegaConf.to_yaml() is used to create a proper format for printing the trainer config\n",
    "print(OmegaConf.to_yaml(config.exp_manager))\n",
    "\n",
    "exp_dir = exp_manager(trainer, config.exp_manager)\n",
    "\n",
    "# the exp_dir provides a path to the current experiment for easy access\n",
    "print(exp_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tjLhUvL_o7_"
   },
   "source": [
    "Before initializing the model, we might want to modify some of the model configs. For example, we might want to modify the pretrained BERT model to another model. The default model is `bert-base-uncased`. We support a variety of models including all the models available in HuggingFace, and Megatron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Xeuc2i7Y_nP5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert-base-uncased', 'bert-large-uncased', 'bert-base-cased', 'bert-large-cased', 'bert-base-multilingual-uncased', 'bert-base-multilingual-cased', 'bert-base-chinese', 'bert-base-german-cased', 'bert-large-uncased-whole-word-masking', 'bert-large-cased-whole-word-masking', 'bert-large-uncased-whole-word-masking-finetuned-squad', 'bert-large-cased-whole-word-masking-finetuned-squad', 'bert-base-cased-finetuned-mrpc', 'bert-base-german-dbmdz-cased', 'bert-base-german-dbmdz-uncased', 'cl-tohoku/bert-base-japanese', 'cl-tohoku/bert-base-japanese-whole-word-masking', 'cl-tohoku/bert-base-japanese-char', 'cl-tohoku/bert-base-japanese-char-whole-word-masking', 'TurkuNLP/bert-base-finnish-cased-v1', 'TurkuNLP/bert-base-finnish-uncased-v1', 'wietsedv/bert-base-dutch-cased', 'distilbert-base-uncased', 'distilbert-base-uncased-distilled-squad', 'distilbert-base-cased', 'distilbert-base-cased-distilled-squad', 'distilbert-base-german-cased', 'distilbert-base-multilingual-cased', 'distilbert-base-uncased-finetuned-sst-2-english', 'camembert-base', 'Musixmatch/umberto-commoncrawl-cased-v1', 'Musixmatch/umberto-wikipedia-uncased-v1', 'roberta-base', 'roberta-large', 'roberta-large-mnli', 'distilroberta-base', 'roberta-base-openai-detector', 'roberta-large-openai-detector', 'albert-base-v1', 'albert-large-v1', 'albert-xlarge-v1', 'albert-xxlarge-v1', 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2', 'albert-xxlarge-v2', 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'distilgpt2']\n"
     ]
    }
   ],
   "source": [
    "# complete list of supported BERT-like models\n",
    "print(nemo_nlp.modules.get_pretrained_lm_models_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RK2xglXyAUOO"
   },
   "outputs": [],
   "source": [
    "# specify the BERT-like model, you want to use\n",
    "# set the `model.language_modelpretrained_model_name' parameter in the config to the model you want to use\n",
    "config.model.language_model.pretrained_model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzNZNAVRjDD-"
   },
   "source": [
    "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders will also be prepared for the training and validation.\n",
    "\n",
    "Also, the pretrained BERT model will be automatically downloaded. Note it can take up to a few minutes depending on the size of the chosen BERT model for the first time you create the model. If your dataset is large, it also may take some time to read and process all the datasets. \n",
    "\n",
    "Now we can create the model with the model config and the trainer object like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NgsGLydWo-6-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:27:27 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: None, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "[NeMo W 2023-01-28 22:27:32 modelPT:222] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:120] Read 962 examples from 2-jamoa uchun/Data/train.txt.\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:239] example 0: ['ekologiya', 'atrof', 'muhitni', 'muhofaza', 'qilish', 'tabiiy', 'resurslardan', 'oqilona', 'foydalanish', 'ularni', 'qayta', 'tiklash', 'va', 'inson', 'faoliyatining', 'tabiatga', 'salbiy', 'taʼsiri', 'oldini', 'olish', 'sohasida', 'yagona', 'davlat', 'siyosatini', 'amalga', 'oshirish', 'zarur']\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:240] subtokens: [CLS] ek ##olo ##gi ##ya at ##ro ##f mu ##hit ##ni mu ##hof ##az ##a qi ##lish tab ##ii ##y res ##urs ##lard ##an o ##qi ##lon ##a f ##oy ##dal ##ani ##sh ul ##ar ##ni q ##ay ##ta ti ##kla ##sh va ins ##on fa ##oli ##yat ##ining tab ##ia ##t ##ga sal ##bi ##y ta ##ʼ ##sir ##i old ##ini ol ##ish so ##has ##ida ya ##gon ##a da ##v ##lat si ##yo ##sat ##ini ama ##lga os ##hir ##ish za ##ru ##r [SEP]\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:241] input_ids: 101 23969 12898 5856 3148 2012 3217 2546 14163 16584 3490 14163 14586 10936 2050 18816 13602 21628 6137 2100 24501 9236 20822 2319 1051 14702 7811 2050 1042 6977 9305 7088 4095 17359 2906 3490 1053 4710 2696 14841 26086 4095 12436 16021 2239 6904 10893 26139 24002 21628 2401 2102 3654 16183 5638 2100 11937 29712 29481 2072 2214 5498 19330 4509 2061 14949 8524 8038 7446 2050 4830 2615 20051 9033 7677 16846 5498 25933 27887 9808 11961 4509 23564 6820 2099 102\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:244] label: 2\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:239] example 1: ['jinoyatchilikning', 'yuqori', 'darajasi', 'hukumat', 'jinoyatchilikning', 'oldini', 'olish', 'dasturlariga', 'sarmoya', 'kiritishi', 'va', 'jinoyatchilik', 'darajasini', 'pasaytirish', 'va', 'jamiyatlarni', 'xavfsizroq', 'qilish', 'uchun', 'politsiya', 'xodimlari', 'va', 'boshqa', 'huquqni', 'muhofaza', 'qilish', 'organlari', 'xodimlari', 'sonini', 'oshirishi', 'mumkin']\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:240] subtokens: [CLS] jin ##oya ##tch ##ili ##k ##ning yu ##q ##ori dar ##aja ##si hu ##ku ##mat jin ##oya ##tch ##ili ##k ##ning old ##ini ol ##ish das ##tur ##lar ##iga sar ##mo ##ya ki ##rit ##ish ##i va jin ##oya ##tch ##ili ##k dar ##aja ##sin ##i pas ##ay ##ti ##rish va jam ##iya ##tl ##ar ##ni x ##av ##fs ##iz ##ro ##q qi ##lish uc ##hun pol ##its ##iya x ##od ##im ##lar ##i va bo ##sh ##qa hu ##qu ##q ##ni mu ##hof ##az ##a qi ##lish organ ##lar ##i x ##od ##im ##lar ##i son ##ini os ##hir ##ish ##i mum ##kin [SEP]\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:241] input_ids: 101 9743 18232 10649 18622 2243 5582 9805 4160 10050 18243 22734 5332 15876 5283 18900 9743 18232 10649 18622 2243 5582 2214 5498 19330 4509 8695 20689 8017 13340 18906 5302 3148 11382 14778 4509 2072 12436 9743 18232 10649 18622 2243 18243 22734 11493 2072 14674 4710 3775 18774 12436 9389 8717 19646 2906 3490 1060 11431 10343 10993 3217 4160 18816 13602 15384 17157 14955 12762 8717 1060 7716 5714 8017 2072 12436 8945 4095 19062 15876 28940 4160 3490 14163 14586 10936 2050 18816 13602 5812 8017 2072 1060 7716 5714 8017 2072 2365 5498 9808 11961 4509 2072 12954 4939 102\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-01-28 22:27:32 text_classification_dataset:244] label: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:27:37 text_classification_dataset:250] Found 468 out of 962 sentences with more than 256 subtokens. Truncated long sentences from the end.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:27:37 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-01-28 22:27:37 data_preprocessing:406] Min: 33 |                  Max: 256 |                  Mean: 185.73076923076923 |                  Median: 246.0\n",
      "[NeMo I 2023-01-28 22:27:37 data_preprocessing:412] 75 percentile: 256.00\n",
      "[NeMo I 2023-01-28 22:27:37 data_preprocessing:413] 99 percentile: 256.00\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:120] Read 9 examples from 2-jamoa uchun/Data/test.txt.\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:239] example 0: ['bugungi', 'kunda', 'mamlakatimizda', 'chuqur', 'demokratik', 'islohotlar', 'amalga', 'oshirilmoqda', 'demokratiyaning', 'asosiy', 'elementlaridan', 'biri', 'bu', 'ommaviy', 'axborot', 'vositalari', 'erkinligi', 'va', 'ular', 'huquqlarining', 'qonun', 'bilan', 'kafolatlanganligi', 'hisoblanadi', 'demokratik', 'islohotlarni', 'yana', 'da', 'mustahkamlash', 'maqsadida', 'ommaviy', 'axborot', 'vositalari', 'xodimlarining', 'qonuniy', 'faoliyatiga', 'aralashganlik', 'va', 'toʻsqinlik', 'qilganlik', 'uchun', 'maʼmuriy', 'va', 'jinoiy', 'javobgarlik', 'joriy', 'qilishni', 'taklif', 'qilamiz', 'buning', 'uchun', 'oʻzbekiston', 'respublikasi', 'maʼmuriy', 'javobgarlik', 'toʻgʻrisidagi', 'va', 'jinoyat', 'kodekslariga', 'oʻzgartish', 'va', 'qoʻshimchalar', 'kiritish', 'zarur', 'zero', 'bugungi', 'islohotlar', 'davrida', 'soʻz', 'va', 'matbuot', 'erkinligining', 'taʼminlanishi', 'davlat', 'va', 'xalq', 'oʻrtasidagi', 'ochiq', 'muloqotni', 'barqaror', 'taraqqiyot', 'va', 'xalq', 'farovonligini', 'taʼminlashda', 'muhim', 'qadam', 'boʻladi', 'toʻrt', 'ming', 'sakkiz', 'yuz', 'olti']\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:240] subtokens: [CLS] bug ##ung ##i kun ##da ma ##ml ##aka ##ti ##mi ##zd ##a chu ##qu ##r demo ##kra ##ti ##k is ##loh ##ot ##lar ama ##lga os ##hir ##il ##mo ##q ##da demo ##kra ##ti ##yan ##ing as ##osi ##y element ##lar ##ida ##n bi ##ri bu om ##ma ##vi ##y ax ##boro ##t vo ##sit ##ala ##ri er ##kin ##li ##gi va ul ##ar hu ##qu ##q ##lar ##ining q ##on ##un bi ##lan ka ##fo ##lat ##lang ##an ##li ##gi his ##ob ##lana ##di demo ##kra ##ti ##k is ##loh ##ot ##lar ##ni yan ##a da must ##ah ##kam ##lash ma ##q ##sa ##di ##da om ##ma ##vi ##y ax ##boro ##t vo ##sit ##ala ##ri x ##od ##im ##lar ##ining q ##on ##uni ##y fa ##oli ##yat ##iga ara ##lash ##gan ##lik va to ##ʻ ##s ##qi ##nl ##ik qi ##lga ##nl ##ik uc ##hun ma ##ʼ ##mur ##iy va jin ##oi ##y ja ##vo ##b ##gar ##lik jo ##ri ##y qi ##lish ##ni tak ##li ##f qi ##lam ##iz bun ##ing uc ##hun o ##ʻ ##zbek ##isto ##n res ##pu ##bl ##ika ##si ma ##ʼ ##mur ##iy ja ##vo ##b ##gar ##lik to ##ʻ ##g ##ʻ ##ris ##ida ##gi va jin ##oya ##t ko ##dek ##sl ##ari ##ga o ##ʻ ##z ##gart ##ish va q ##o ##ʻ ##shi ##mc ##hala ##r ki ##rit ##ish za ##ru ##r zero bug ##ung ##i is ##loh ##ot ##lar da ##vr ##ida so ##ʻ ##z va mat ##bu ##ot er ##kin ##li ##gin ##ing ta [SEP]\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:241] input_ids: 101 11829 5575 2072 28919 2850 5003 19968 11905 3775 4328 26494 2050 14684 28940 2099 9703 22272 3775 2243 2003 24729 4140 8017 25933 27887 9808 11961 4014 5302 4160 2850 9703 22272 3775 7054 2075 2004 20049 2100 5783 8017 8524 2078 12170 3089 20934 18168 2863 5737 2100 22260 12691 2102 29536 28032 7911 3089 9413 4939 3669 5856 12436 17359 2906 15876 28940 4160 8017 24002 1053 2239 4609 12170 5802 10556 14876 20051 25023 2319 3669 5856 2010 16429 16695 4305 9703 22272 3775 2243 2003 24729 4140 8017 3490 13619 2050 4830 2442 4430 27052 27067 5003 4160 3736 4305 2850 18168 2863 5737 2100 22260 12691 2102 29536 28032 7911 3089 1060 7716 5714 8017 24002 1053 2239 19496 2100 6904 10893 26139 13340 19027 27067 5289 18393 12436 2000 29711 2015 14702 20554 5480 18816 27887 20554 5480 15384 17157 5003 29712 20136 28008 12436 9743 10448 2100 14855 6767 2497 6843 18393 8183 3089 2100 18816 13602 3490 27006 3669 2546 18816 10278 10993 21122 2075 15384 17157 1051 29711 28733 20483 2078 24501 14289 16558 7556 5332 5003 29712 20136 28008 14855 6767 2497 6843 18393 2000 29711 2290 29711 6935 8524 5856 12436 9743 18232 2102 12849 24463 14540 8486 3654 1051 29711 2480 27378 4509 12436 1053 2080 29711 6182 12458 19531 2099 11382 14778 4509 23564 6820 2099 5717 11829 5575 2072 2003 24729 4140 8017 4830 19716 8524 2061 29711 2480 12436 13523 8569 4140 9413 4939 3669 11528 2075 11937 102\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:239] example 1: ['hozirgi', 'kunda', 'fuqarolar', 'yangi', 'boʻlmagan', 'avtomobillarni', 'sotishda', 'va', 'xarid', 'qilishda', 'majburiy', 'ravishda', 'notariuslarga', 'borishga', 'majbur', 'boʻlishmoqda', 'hammaga', 'maʼlumki', 'bugungi', 'kunda', 'mavjud', 'notarial', 'idoralar', 'kamligi', 'sababli', 'oldi', 'sotdi', 'shartnomalarni', 'rasmiylashtirish', 'koʻp', 'vaqtni', 'olmoqda', 'yangi', 'boʻlmagan', 'avtomobillarning', 'oldi', 'sotdi', 'shartnomalarini', 'rasmiylashtirish', 'va', 'yoʻl', 'harakati', 'xavfsizligi', 'boʻlimida', 'roʻyxatga', 'olishdagi', 'toʻlovlar', 'qimmatdir', 'shu', 'sababli', 'avtomobillarni', 'oldi', 'sotdi', 'shartnomalarini', 'notariusda', 'rasmiylashtirishni', 'bekor', 'qilish', 'lozim', 'dunyoning', 'koʻpgina', 'davlatlarida', 'avtomobillarni', 'oldi', 'sotdi', 'va', 'roʻyxatga', 'qoʻyish', 'jarayonlari', 'soddalashtirilgan', 'masalan', 'yangi', 'boʻlmagan', 'avtomobillarni', 'sotishda', 'fuqarolar', 'oʻzaro', 'oldi', 'sotdi', 'shartnomasini', 'tuzib', 'imzolashadi', 'shundan', 'soʻng', 'yoʻl', 'harakati', 'xavfsizligi', 'boʻlimida', 'xarid', 'qilingan', 'avtomobilni', 'roʻyxatga', 'qoʻyishadi', 'bundan', 'tashqari', 'ushbu', 'davlat', 'xizmatlari', 'fuqarolar', 'uchun', 'arzon', 'narxlarda', 'koʻrsatiladi', 'shu', 'sababdan', 'yangi', 'boʻlmagan', 'avtomobillarni', 'oldi', 'sotdi', 'shartnomalarini', 'rasmiylashtirishni', 'soddalashtirish', 'va', 'yoʻl', 'harakati', 'xavfsizligi', 'boʻlimida', 'roʻyxatga', 'olishdagi', 'toʻlovlarni', 'ham', 'arzon', 'qilish', 'lozim', 'toʻrt', 'ming', 'toʻrt', 'yuz', 'yetmish', 'bir']\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:240] subtokens: [CLS] ho ##zi ##rg ##i kun ##da fu ##qa ##rol ##ar yang ##i bo ##ʻ ##lma ##gan av ##tom ##ob ##illa ##rn ##i so ##tish ##da va x ##ari ##d qi ##lish ##da maj ##bu ##ri ##y ravi ##sh ##da not ##ari ##us ##lar ##ga boris ##hg ##a maj ##bu ##r bo ##ʻ ##lish ##mo ##q ##da ham ##ma ##ga ma ##ʼ ##lum ##ki bug ##ung ##i kun ##da ma ##v ##ju ##d not ##aria ##l id ##ora ##lar kam ##li ##gi sa ##ba ##bl ##i old ##i so ##t ##di sha ##rt ##no ##mal ##ar ##ni ras ##mi ##yla ##sh ##ti ##rish ko ##ʻ ##p va ##q ##t ##ni ol ##mo ##q ##da yang ##i bo ##ʻ ##lma ##gan av ##tom ##ob ##illa ##rn ##ing old ##i so ##t ##di sha ##rt ##no ##mal ##ari ##ni ras ##mi ##yla ##sh ##ti ##rish va yo ##ʻ ##l hara ##kat ##i x ##av ##fs ##iz ##li ##gi bo ##ʻ ##lim ##ida ro ##ʻ ##yx ##at ##ga ol ##ish ##da ##gi to ##ʻ ##lov ##lar qi ##mma ##t ##di ##r shu sa ##ba ##bl ##i av ##tom ##ob ##illa ##rn ##i old ##i so ##t ##di sha ##rt ##no ##mal ##ari ##ni not ##ari ##us ##da ras ##mi ##yla ##sh ##ti ##rish ##ni be ##kor qi ##lish lo ##zi ##m dun ##yon ##ing ko ##ʻ ##pg ##ina da ##v ##lat ##lar ##ida av ##tom ##ob ##illa ##rn ##i old ##i so ##t ##di va ro ##ʻ ##yx ##at ##ga q ##o ##ʻ ##yi ##sh jar ##ayo ##nl [SEP]\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:241] input_ids: 101 7570 5831 10623 2072 28919 2850 11865 19062 13153 2906 8675 2072 8945 29711 19145 5289 20704 20389 16429 9386 6826 2072 2061 24788 2850 12436 1060 8486 2094 18816 13602 2850 16686 8569 3089 2100 16806 4095 2850 2025 8486 2271 8017 3654 11235 25619 2050 16686 8569 2099 8945 29711 13602 5302 4160 2850 10654 2863 3654 5003 29712 12942 3211 11829 5575 2072 28919 2850 5003 2615 9103 2094 2025 10980 2140 8909 6525 8017 27829 3669 5856 7842 3676 16558 2072 2214 2072 2061 2102 4305 21146 5339 3630 9067 2906 3490 20710 4328 23943 4095 3775 18774 12849 29711 2361 12436 4160 2102 3490 19330 5302 4160 2850 8675 2072 8945 29711 19145 5289 20704 20389 16429 9386 6826 2075 2214 2072 2061 2102 4305 21146 5339 3630 9067 8486 3490 20710 4328 23943 4095 3775 18774 12436 10930 29711 2140 18820 24498 2072 1060 11431 10343 10993 3669 5856 8945 29711 17960 8524 20996 29711 17275 4017 3654 19330 4509 2850 5856 2000 29711 14301 8017 18816 14760 2102 4305 2099 18454 7842 3676 16558 2072 20704 20389 16429 9386 6826 2072 2214 2072 2061 2102 4305 21146 5339 3630 9067 8486 3490 2025 8486 2271 2850 20710 4328 23943 4095 3775 18774 3490 2022 21815 18816 13602 8840 5831 2213 24654 14001 2075 12849 29711 26952 3981 4830 2615 20051 8017 8524 20704 20389 16429 9386 6826 2072 2214 2072 2061 2102 4305 12436 20996 29711 17275 4017 3654 1053 2080 29711 10139 4095 15723 28852 20554 102\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_dataset:244] label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:27:37 text_classification_dataset:250] Found 9 out of 9 sentences with more than 256 subtokens. Truncated long sentences from the end.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:27:37 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-01-28 22:27:37 data_preprocessing:406] Min: 256 |                  Max: 256 |                  Mean: 256.0 |                  Median: 256.0\n",
      "[NeMo I 2023-01-28 22:27:37 data_preprocessing:412] 75 percentile: 256.00\n",
      "[NeMo I 2023-01-28 22:27:37 data_preprocessing:413] 99 percentile: 256.00\n",
      "[NeMo I 2023-01-28 22:27:37 text_classification_model:193] Dataloader config or file_path for the test is missing, so no data loader for test is created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:27:37 nlp_overrides:229] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
      "    Megatron-based models require Apex to function correctly.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = nemo_nlp.models.TextClassificationModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MW_JVIi5z68e"
   },
   "source": [
    "## Training\n",
    "\n",
    "You may start the training by using the trainer.fit() method. The number of steps/epochs of the training are specified already in the config of the trainer and you may update them before creating the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hUvnSpyjp0Dh",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:28:32 modelPT:602] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        capturable: False\n",
      "        differentiable: False\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        fused: False\n",
      "        lr: 2e-05\n",
      "        maximize: False\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2023-01-28 22:28:32 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7f0fe42ff640>\" \n",
      "    will be used during training (effective maximum steps = 1210) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 1210\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | loss                  | CrossEntropyLoss     | 0     \n",
      "1 | bert_model            | BertEncoder          | 109 M \n",
      "2 | classifier            | SequenceClassifier   | 597 K \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.319   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011664628982543945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9f0742a11c4611a3062a569e50f379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:28:32 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:28:32 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:28:32 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014616250991821289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108105a13e464e47801d54d5fdbdc3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:29:02 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:29:02 text_classification_model:142] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00          1\n",
      "    label_id: 1                                              0.00       0.00       0.00          1\n",
      "    label_id: 2                                              0.00       0.00       0.00          1\n",
      "    label_id: 3                                              0.00       0.00       0.00          1\n",
      "    label_id: 4                                              0.00       0.00       0.00          1\n",
      "    label_id: 5                                              0.00       0.00       0.00          1\n",
      "    label_id: 6                                              0.00       0.00       0.00          1\n",
      "    label_id: 7                                              0.00       0.00       0.00          1\n",
      "    label_id: 8                                             11.11     100.00      20.00          1\n",
      "    -------------------\n",
      "    micro avg                                               11.11      11.11      11.11          9\n",
      "    macro avg                                                1.23      11.11       2.22          9\n",
      "    weighted avg                                             1.23      11.11       2.22          9\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 121: 'val_loss' reached 2.33134 (best 2.33134), saving model to '/home/airi/Desktop/jupyter/til model/text classification/nemo_experiments/TextClassification/2023-01-28_22-27-15/checkpoints/TextClassification--val_loss=2.3313-epoch=0.ckpt' as top 3\n",
      "[NeMo W 2023-01-28 22:29:05 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:29:05 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:29:05 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010899782180786133,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06386a037fc44a5ab1bb12cb8418f17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:29:34 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:29:34 text_classification_model:142] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00          1\n",
      "    label_id: 1                                              0.00       0.00       0.00          1\n",
      "    label_id: 2                                              0.00       0.00       0.00          1\n",
      "    label_id: 3                                              0.00       0.00       0.00          1\n",
      "    label_id: 4                                              0.00       0.00       0.00          1\n",
      "    label_id: 5                                              0.00       0.00       0.00          1\n",
      "    label_id: 6                                              0.00       0.00       0.00          1\n",
      "    label_id: 7                                            100.00     100.00     100.00          1\n",
      "    label_id: 8                                              0.00       0.00       0.00          1\n",
      "    -------------------\n",
      "    micro avg                                               11.11      11.11      11.11          9\n",
      "    macro avg                                               11.11      11.11      11.11          9\n",
      "    weighted avg                                            11.11      11.11      11.11          9\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 242: 'val_loss' reached 2.18094 (best 2.18094), saving model to '/home/airi/Desktop/jupyter/til model/text classification/nemo_experiments/TextClassification/2023-01-28_22-27-15/checkpoints/TextClassification--val_loss=2.1809-epoch=1.ckpt' as top 3\n",
      "[NeMo W 2023-01-28 22:29:37 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:29:37 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:29:37 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013250350952148438,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e41659819b4fc7aa48183b87e1dcf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:30:06 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:30:06 text_classification_model:142] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00          1\n",
      "    label_id: 1                                              0.00       0.00       0.00          1\n",
      "    label_id: 2                                              0.00       0.00       0.00          1\n",
      "    label_id: 3                                              0.00       0.00       0.00          1\n",
      "    label_id: 4                                              0.00       0.00       0.00          1\n",
      "    label_id: 5                                              0.00       0.00       0.00          1\n",
      "    label_id: 6                                              0.00       0.00       0.00          1\n",
      "    label_id: 7                                            100.00     100.00     100.00          1\n",
      "    label_id: 8                                             14.29     100.00      25.00          1\n",
      "    -------------------\n",
      "    micro avg                                               22.22      22.22      22.22          9\n",
      "    macro avg                                               12.70      22.22      13.89          9\n",
      "    weighted avg                                            12.70      22.22      13.89          9\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 363: 'val_loss' reached 2.20097 (best 2.18094), saving model to '/home/airi/Desktop/jupyter/til model/text classification/nemo_experiments/TextClassification/2023-01-28_22-27-15/checkpoints/TextClassification--val_loss=2.2010-epoch=2.ckpt' as top 3\n",
      "[NeMo W 2023-01-28 22:30:09 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:30:09 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:30:09 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010605812072753906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a88eae8e764ab081aa904cbbd9bc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:30:38 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:30:38 text_classification_model:142] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00          1\n",
      "    label_id: 1                                              0.00       0.00       0.00          1\n",
      "    label_id: 2                                              0.00       0.00       0.00          1\n",
      "    label_id: 3                                              0.00       0.00       0.00          1\n",
      "    label_id: 4                                              0.00       0.00       0.00          1\n",
      "    label_id: 5                                              0.00       0.00       0.00          1\n",
      "    label_id: 6                                            100.00     100.00     100.00          1\n",
      "    label_id: 7                                            100.00     100.00     100.00          1\n",
      "    label_id: 8                                             20.00     100.00      33.33          1\n",
      "    -------------------\n",
      "    micro avg                                               33.33      33.33      33.33          9\n",
      "    macro avg                                               24.44      33.33      25.93          9\n",
      "    weighted avg                                            24.44      33.33      25.93          9\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 484: 'val_loss' reached 2.07827 (best 2.07827), saving model to '/home/airi/Desktop/jupyter/til model/text classification/nemo_experiments/TextClassification/2023-01-28_22-27-15/checkpoints/TextClassification--val_loss=2.0783-epoch=3.ckpt' as top 3\n",
      "[NeMo W 2023-01-28 22:30:41 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:30:41 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:30:41 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010766029357910156,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53de8193d7264cc1b6e9299e60fdd7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:31:10 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:31:10 text_classification_model:142] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00          1\n",
      "    label_id: 1                                              0.00       0.00       0.00          1\n",
      "    label_id: 2                                              0.00       0.00       0.00          1\n",
      "    label_id: 3                                              0.00       0.00       0.00          1\n",
      "    label_id: 4                                              0.00       0.00       0.00          1\n",
      "    label_id: 5                                              0.00       0.00       0.00          1\n",
      "    label_id: 6                                            100.00     100.00     100.00          1\n",
      "    label_id: 7                                            100.00     100.00     100.00          1\n",
      "    label_id: 8                                             20.00     100.00      33.33          1\n",
      "    -------------------\n",
      "    micro avg                                               33.33      33.33      33.33          9\n",
      "    macro avg                                               24.44      33.33      25.93          9\n",
      "    weighted avg                                            24.44      33.33      25.93          9\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 605: 'val_loss' reached 2.05235 (best 2.05235), saving model to '/home/airi/Desktop/jupyter/til model/text classification/nemo_experiments/TextClassification/2023-01-28_22-27-15/checkpoints/TextClassification--val_loss=2.0523-epoch=4.ckpt' as top 3\n",
      "[NeMo W 2023-01-28 22:31:13 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:31:13 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:31:13 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012452363967895508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4d7b6e6358424ab740e7f390658c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:31:42 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:31:42 text_classification_model:142] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00          1\n",
      "    label_id: 1                                              0.00       0.00       0.00          1\n",
      "    label_id: 2                                              0.00       0.00       0.00          1\n",
      "    label_id: 3                                              0.00       0.00       0.00          1\n",
      "    label_id: 4                                              0.00       0.00       0.00          1\n",
      "    label_id: 5                                              0.00       0.00       0.00          1\n",
      "    label_id: 6                                            100.00     100.00     100.00          1\n",
      "    label_id: 7                                            100.00     100.00     100.00          1\n",
      "    label_id: 8                                             25.00     100.00      40.00          1\n",
      "    -------------------\n",
      "    micro avg                                               33.33      33.33      33.33          9\n",
      "    macro avg                                               25.00      33.33      26.67          9\n",
      "    weighted avg                                            25.00      33.33      26.67          9\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 726: 'val_loss' reached 1.88256 (best 1.88256), saving model to '/home/airi/Desktop/jupyter/til model/text classification/nemo_experiments/TextClassification/2023-01-28_22-27-15/checkpoints/TextClassification--val_loss=1.8826-epoch=5.ckpt' as top 3\n",
      "[NeMo W 2023-01-28 22:31:46 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:31:46 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:31:46 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01372218132019043,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703b0d7fa83f4d3e83ca9de606da4b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:32:15 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:32:15 text_classification_model:142] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00          1\n",
      "    label_id: 1                                              0.00       0.00       0.00          1\n",
      "    label_id: 2                                              0.00       0.00       0.00          1\n",
      "    label_id: 3                                              0.00       0.00       0.00          1\n",
      "    label_id: 4                                              0.00       0.00       0.00          1\n",
      "    label_id: 5                                              0.00       0.00       0.00          1\n",
      "    label_id: 6                                            100.00     100.00     100.00          1\n",
      "    label_id: 7                                            100.00     100.00     100.00          1\n",
      "    label_id: 8                                             20.00     100.00      33.33          1\n",
      "    -------------------\n",
      "    micro avg                                               33.33      33.33      33.33          9\n",
      "    macro avg                                               24.44      33.33      25.93          9\n",
      "    weighted avg                                            24.44      33.33      25.93          9\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 847: 'val_loss' reached 1.83381 (best 1.83381), saving model to '/home/airi/Desktop/jupyter/til model/text classification/nemo_experiments/TextClassification/2023-01-28_22-27-15/checkpoints/TextClassification--val_loss=1.8338-epoch=6.ckpt' as top 3\n",
      "[NeMo W 2023-01-28 22:32:18 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:32:18 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:32:18 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012876749038696289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9349704963fb47529132137bd9caf2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:32:47 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:32:47 text_classification_model:142] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00          1\n",
      "    label_id: 1                                              0.00       0.00       0.00          1\n",
      "    label_id: 2                                              0.00       0.00       0.00          1\n",
      "    label_id: 3                                              0.00       0.00       0.00          1\n",
      "    label_id: 4                                              0.00       0.00       0.00          1\n",
      "    label_id: 5                                              0.00       0.00       0.00          1\n",
      "    label_id: 6                                            100.00     100.00     100.00          1\n",
      "    label_id: 7                                            100.00     100.00     100.00          1\n",
      "    label_id: 8                                             25.00     100.00      40.00          1\n",
      "    -------------------\n",
      "    micro avg                                               33.33      33.33      33.33          9\n",
      "    macro avg                                               25.00      33.33      26.67          9\n",
      "    weighted avg                                            25.00      33.33      26.67          9\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 968: 'val_loss' reached 1.87552 (best 1.83381), saving model to '/home/airi/Desktop/jupyter/til model/text classification/nemo_experiments/TextClassification/2023-01-28_22-27-15/checkpoints/TextClassification--val_loss=1.8755-epoch=7.ckpt' as top 3\n",
      "[NeMo W 2023-01-28 22:32:50 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:32:50 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:32:50 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010760307312011719,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee19be7aa8ab4c57af22df27c69f471a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:33:19 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:33:19 text_classification_model:142] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00          1\n",
      "    label_id: 1                                              0.00       0.00       0.00          1\n",
      "    label_id: 2                                              0.00       0.00       0.00          1\n",
      "    label_id: 3                                              0.00       0.00       0.00          1\n",
      "    label_id: 4                                              0.00       0.00       0.00          1\n",
      "    label_id: 5                                             50.00     100.00      66.67          1\n",
      "    label_id: 6                                            100.00     100.00     100.00          1\n",
      "    label_id: 7                                            100.00     100.00     100.00          1\n",
      "    label_id: 8                                             33.33     100.00      50.00          1\n",
      "    -------------------\n",
      "    micro avg                                               44.44      44.44      44.44          9\n",
      "    macro avg                                               31.48      44.44      35.19          9\n",
      "    weighted avg                                            31.48      44.44      35.19          9\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1089: 'val_loss' reached 1.64826 (best 1.64826), saving model to '/home/airi/Desktop/jupyter/til model/text classification/nemo_experiments/TextClassification/2023-01-28_22-27-15/checkpoints/TextClassification--val_loss=1.6483-epoch=8.ckpt' as top 3\n",
      "[NeMo W 2023-01-28 22:33:22 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:33:22 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2023-01-28 22:33:22 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013002634048461914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3054f2be497845fd931ab8c1fe30f56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:33:51 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:33:51 text_classification_model:142] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00          1\n",
      "    label_id: 1                                              0.00       0.00       0.00          1\n",
      "    label_id: 2                                              0.00       0.00       0.00          1\n",
      "    label_id: 3                                              0.00       0.00       0.00          1\n",
      "    label_id: 4                                              0.00       0.00       0.00          1\n",
      "    label_id: 5                                             50.00     100.00      66.67          1\n",
      "    label_id: 6                                            100.00     100.00     100.00          1\n",
      "    label_id: 7                                            100.00     100.00     100.00          1\n",
      "    label_id: 8                                             33.33     100.00      50.00          1\n",
      "    -------------------\n",
      "    micro avg                                               44.44      44.44      44.44          9\n",
      "    macro avg                                               31.48      44.44      35.19          9\n",
      "    weighted avg                                            31.48      44.44      35.19          9\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1210: 'val_loss' reached 1.58924 (best 1.58924), saving model to '/home/airi/Desktop/jupyter/til model/text classification/nemo_experiments/TextClassification/2023-01-28_22-27-15/checkpoints/TextClassification--val_loss=1.5892-epoch=9.ckpt' as top 3\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "# start model training\n",
    "trainer.fit(model)\n",
    "model.save_to(config.save_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPdzJVAgSFaJ"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "To see how the model performs, we can run evaluate and test the performance of the trained model on a data file. Here we would load the best checkpoint (the one with the lowest validation loss) and create a model (eval_model) from the checkpoint. We would also create a new trainer (eval_trainer) to show how it is done when training is done and you have just the checkpoints. If you want to perform the evaluation in the same script as the training's script, you may still use the same model and trainer you used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "92PB0iTqNnW-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:50:17 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/cloud_io.py:41: LightningDeprecationWarning: `pytorch_lightning.utilities.cloud_io.load` has been deprecated in v1.8.0 and will be removed in v1.10.0. This function is internal but you can copy over its implementation.\n",
      "      rank_zero_deprecation(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:50:17 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /home/airi/.cache/huggingface/nemo_nlp_tmp/ae0d012864bdb2474ba67537c3f5e0fa/vocab.txt, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "[NeMo W 2023-01-28 22:50:23 modelPT:222] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo W 2023-01-28 22:50:23 modelPT:142] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    file_path: 2-jamoa uchun/Data/train.txt\n",
      "    batch_size: 8\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2023-01-28 22:50:23 modelPT:149] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    file_path: 2-jamoa uchun/Data/test.txt\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2023-01-28 22:50:23 modelPT:155] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2023-01-28 22:50:23 nlp_overrides:229] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
      "    Megatron-based models require Apex to function correctly.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo W 2023-01-28 22:50:27 modelPT:222] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:120] Read 9 examples from 2-jamoa uchun/Data/test.txt.\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:239] example 0: ['bugungi', 'kunda', 'mamlakatimizda', 'chuqur', 'demokratik', 'islohotlar', 'amalga', 'oshirilmoqda', 'demokratiyaning', 'asosiy', 'elementlaridan', 'biri', 'bu', 'ommaviy', 'axborot', 'vositalari', 'erkinligi', 'va', 'ular', 'huquqlarining', 'qonun', 'bilan', 'kafolatlanganligi', 'hisoblanadi', 'demokratik', 'islohotlarni', 'yana', 'da', 'mustahkamlash', 'maqsadida', 'ommaviy', 'axborot', 'vositalari', 'xodimlarining', 'qonuniy', 'faoliyatiga', 'aralashganlik', 'va', 'toʻsqinlik', 'qilganlik', 'uchun', 'maʼmuriy', 'va', 'jinoiy', 'javobgarlik', 'joriy', 'qilishni', 'taklif', 'qilamiz', 'buning', 'uchun', 'oʻzbekiston', 'respublikasi', 'maʼmuriy', 'javobgarlik', 'toʻgʻrisidagi', 'va', 'jinoyat', 'kodekslariga', 'oʻzgartish', 'va', 'qoʻshimchalar', 'kiritish', 'zarur', 'zero', 'bugungi', 'islohotlar', 'davrida', 'soʻz', 'va', 'matbuot', 'erkinligining', 'taʼminlanishi', 'davlat', 'va', 'xalq', 'oʻrtasidagi', 'ochiq', 'muloqotni', 'barqaror', 'taraqqiyot', 'va', 'xalq', 'farovonligini', 'taʼminlashda', 'muhim', 'qadam', 'boʻladi', 'toʻrt', 'ming', 'sakkiz', 'yuz', 'olti']\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:240] subtokens: [CLS] bug ##ung ##i kun ##da ma ##ml ##aka ##ti ##mi ##zd ##a chu ##qu ##r demo ##kra ##ti ##k is ##loh ##ot ##lar ama ##lga os ##hir ##il ##mo ##q ##da demo ##kra ##ti ##yan ##ing as ##osi ##y element ##lar ##ida ##n bi ##ri bu om ##ma ##vi ##y ax ##boro ##t vo ##sit ##ala ##ri er ##kin ##li ##gi va ul ##ar hu ##qu ##q ##lar ##ining q ##on ##un bi ##lan ka ##fo ##lat ##lang ##an ##li ##gi his ##ob ##lana ##di demo ##kra ##ti ##k is ##loh ##ot ##lar ##ni yan ##a da must ##ah ##kam ##lash ma ##q ##sa ##di ##da om ##ma ##vi ##y ax ##boro ##t vo ##sit ##ala ##ri x ##od ##im ##lar ##ining q ##on ##uni ##y fa ##oli ##yat ##iga ara ##lash ##gan ##lik va to ##ʻ ##s ##qi ##nl ##ik qi ##lga ##nl ##ik uc ##hun ma ##ʼ ##mur ##iy va jin ##oi ##y ja ##vo ##b ##gar ##lik jo ##ri ##y qi ##lish ##ni tak ##li ##f qi ##lam ##iz bun ##ing uc ##hun o ##ʻ ##zbek ##isto ##n res ##pu ##bl ##ika ##si ma ##ʼ ##mur ##iy ja ##vo ##b ##gar ##lik to ##ʻ ##g ##ʻ ##ris ##ida ##gi va jin ##oya ##t ko ##dek ##sl ##ari ##ga o ##ʻ ##z ##gart ##ish va q ##o ##ʻ ##shi ##mc ##hala ##r ki ##rit ##ish za ##ru ##r zero bug ##ung ##i is ##loh ##ot ##lar da ##vr ##ida so ##ʻ ##z va mat ##bu ##ot er ##kin ##li ##gin ##ing ta [SEP]\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:241] input_ids: 101 11829 5575 2072 28919 2850 5003 19968 11905 3775 4328 26494 2050 14684 28940 2099 9703 22272 3775 2243 2003 24729 4140 8017 25933 27887 9808 11961 4014 5302 4160 2850 9703 22272 3775 7054 2075 2004 20049 2100 5783 8017 8524 2078 12170 3089 20934 18168 2863 5737 2100 22260 12691 2102 29536 28032 7911 3089 9413 4939 3669 5856 12436 17359 2906 15876 28940 4160 8017 24002 1053 2239 4609 12170 5802 10556 14876 20051 25023 2319 3669 5856 2010 16429 16695 4305 9703 22272 3775 2243 2003 24729 4140 8017 3490 13619 2050 4830 2442 4430 27052 27067 5003 4160 3736 4305 2850 18168 2863 5737 2100 22260 12691 2102 29536 28032 7911 3089 1060 7716 5714 8017 24002 1053 2239 19496 2100 6904 10893 26139 13340 19027 27067 5289 18393 12436 2000 29711 2015 14702 20554 5480 18816 27887 20554 5480 15384 17157 5003 29712 20136 28008 12436 9743 10448 2100 14855 6767 2497 6843 18393 8183 3089 2100 18816 13602 3490 27006 3669 2546 18816 10278 10993 21122 2075 15384 17157 1051 29711 28733 20483 2078 24501 14289 16558 7556 5332 5003 29712 20136 28008 14855 6767 2497 6843 18393 2000 29711 2290 29711 6935 8524 5856 12436 9743 18232 2102 12849 24463 14540 8486 3654 1051 29711 2480 27378 4509 12436 1053 2080 29711 6182 12458 19531 2099 11382 14778 4509 23564 6820 2099 5717 11829 5575 2072 2003 24729 4140 8017 4830 19716 8524 2061 29711 2480 12436 13523 8569 4140 9413 4939 3669 11528 2075 11937 102\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:239] example 1: ['hozirgi', 'kunda', 'fuqarolar', 'yangi', 'boʻlmagan', 'avtomobillarni', 'sotishda', 'va', 'xarid', 'qilishda', 'majburiy', 'ravishda', 'notariuslarga', 'borishga', 'majbur', 'boʻlishmoqda', 'hammaga', 'maʼlumki', 'bugungi', 'kunda', 'mavjud', 'notarial', 'idoralar', 'kamligi', 'sababli', 'oldi', 'sotdi', 'shartnomalarni', 'rasmiylashtirish', 'koʻp', 'vaqtni', 'olmoqda', 'yangi', 'boʻlmagan', 'avtomobillarning', 'oldi', 'sotdi', 'shartnomalarini', 'rasmiylashtirish', 'va', 'yoʻl', 'harakati', 'xavfsizligi', 'boʻlimida', 'roʻyxatga', 'olishdagi', 'toʻlovlar', 'qimmatdir', 'shu', 'sababli', 'avtomobillarni', 'oldi', 'sotdi', 'shartnomalarini', 'notariusda', 'rasmiylashtirishni', 'bekor', 'qilish', 'lozim', 'dunyoning', 'koʻpgina', 'davlatlarida', 'avtomobillarni', 'oldi', 'sotdi', 'va', 'roʻyxatga', 'qoʻyish', 'jarayonlari', 'soddalashtirilgan', 'masalan', 'yangi', 'boʻlmagan', 'avtomobillarni', 'sotishda', 'fuqarolar', 'oʻzaro', 'oldi', 'sotdi', 'shartnomasini', 'tuzib', 'imzolashadi', 'shundan', 'soʻng', 'yoʻl', 'harakati', 'xavfsizligi', 'boʻlimida', 'xarid', 'qilingan', 'avtomobilni', 'roʻyxatga', 'qoʻyishadi', 'bundan', 'tashqari', 'ushbu', 'davlat', 'xizmatlari', 'fuqarolar', 'uchun', 'arzon', 'narxlarda', 'koʻrsatiladi', 'shu', 'sababdan', 'yangi', 'boʻlmagan', 'avtomobillarni', 'oldi', 'sotdi', 'shartnomalarini', 'rasmiylashtirishni', 'soddalashtirish', 'va', 'yoʻl', 'harakati', 'xavfsizligi', 'boʻlimida', 'roʻyxatga', 'olishdagi', 'toʻlovlarni', 'ham', 'arzon', 'qilish', 'lozim', 'toʻrt', 'ming', 'toʻrt', 'yuz', 'yetmish', 'bir']\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:240] subtokens: [CLS] ho ##zi ##rg ##i kun ##da fu ##qa ##rol ##ar yang ##i bo ##ʻ ##lma ##gan av ##tom ##ob ##illa ##rn ##i so ##tish ##da va x ##ari ##d qi ##lish ##da maj ##bu ##ri ##y ravi ##sh ##da not ##ari ##us ##lar ##ga boris ##hg ##a maj ##bu ##r bo ##ʻ ##lish ##mo ##q ##da ham ##ma ##ga ma ##ʼ ##lum ##ki bug ##ung ##i kun ##da ma ##v ##ju ##d not ##aria ##l id ##ora ##lar kam ##li ##gi sa ##ba ##bl ##i old ##i so ##t ##di sha ##rt ##no ##mal ##ar ##ni ras ##mi ##yla ##sh ##ti ##rish ko ##ʻ ##p va ##q ##t ##ni ol ##mo ##q ##da yang ##i bo ##ʻ ##lma ##gan av ##tom ##ob ##illa ##rn ##ing old ##i so ##t ##di sha ##rt ##no ##mal ##ari ##ni ras ##mi ##yla ##sh ##ti ##rish va yo ##ʻ ##l hara ##kat ##i x ##av ##fs ##iz ##li ##gi bo ##ʻ ##lim ##ida ro ##ʻ ##yx ##at ##ga ol ##ish ##da ##gi to ##ʻ ##lov ##lar qi ##mma ##t ##di ##r shu sa ##ba ##bl ##i av ##tom ##ob ##illa ##rn ##i old ##i so ##t ##di sha ##rt ##no ##mal ##ari ##ni not ##ari ##us ##da ras ##mi ##yla ##sh ##ti ##rish ##ni be ##kor qi ##lish lo ##zi ##m dun ##yon ##ing ko ##ʻ ##pg ##ina da ##v ##lat ##lar ##ida av ##tom ##ob ##illa ##rn ##i old ##i so ##t ##di va ro ##ʻ ##yx ##at ##ga q ##o ##ʻ ##yi ##sh jar ##ayo ##nl [SEP]\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:241] input_ids: 101 7570 5831 10623 2072 28919 2850 11865 19062 13153 2906 8675 2072 8945 29711 19145 5289 20704 20389 16429 9386 6826 2072 2061 24788 2850 12436 1060 8486 2094 18816 13602 2850 16686 8569 3089 2100 16806 4095 2850 2025 8486 2271 8017 3654 11235 25619 2050 16686 8569 2099 8945 29711 13602 5302 4160 2850 10654 2863 3654 5003 29712 12942 3211 11829 5575 2072 28919 2850 5003 2615 9103 2094 2025 10980 2140 8909 6525 8017 27829 3669 5856 7842 3676 16558 2072 2214 2072 2061 2102 4305 21146 5339 3630 9067 2906 3490 20710 4328 23943 4095 3775 18774 12849 29711 2361 12436 4160 2102 3490 19330 5302 4160 2850 8675 2072 8945 29711 19145 5289 20704 20389 16429 9386 6826 2075 2214 2072 2061 2102 4305 21146 5339 3630 9067 8486 3490 20710 4328 23943 4095 3775 18774 12436 10930 29711 2140 18820 24498 2072 1060 11431 10343 10993 3669 5856 8945 29711 17960 8524 20996 29711 17275 4017 3654 19330 4509 2850 5856 2000 29711 14301 8017 18816 14760 2102 4305 2099 18454 7842 3676 16558 2072 20704 20389 16429 9386 6826 2072 2214 2072 2061 2102 4305 21146 5339 3630 9067 8486 3490 2025 8486 2271 2850 20710 4328 23943 4095 3775 18774 3490 2022 21815 18816 13602 8840 5831 2213 24654 14001 2075 12849 29711 26952 3981 4830 2615 20051 8017 8524 20704 20389 16429 9386 6826 2072 2214 2072 2061 2102 4305 12436 20996 29711 17275 4017 3654 1053 2080 29711 10139 4095 15723 28852 20554 102\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-01-28 22:50:27 text_classification_dataset:244] label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:50:27 text_classification_dataset:250] Found 9 out of 9 sentences with more than 256 subtokens. Truncated long sentences from the end.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:50:27 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-01-28 22:50:27 data_preprocessing:406] Min: 256 |                  Max: 256 |                  Mean: 256.0 |                  Median: 256.0\n",
      "[NeMo I 2023-01-28 22:50:27 data_preprocessing:412] 75 percentile: 256.00\n",
      "[NeMo I 2023-01-28 22:50:27 data_preprocessing:413] 99 percentile: 256.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[NeMo W 2023-01-28 22:50:27 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010322332382202148,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Testing",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e98312b40d46fa8111e55ce6b0031e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-28 22:50:27 nemo_logging:349] /home/airi/anaconda3/lib/python3.9/site-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-01-28 22:50:27 text_classification_model:142] test_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                              0.00       0.00       0.00          1\n",
      "    label_id: 1                                              0.00       0.00       0.00          1\n",
      "    label_id: 2                                              0.00       0.00       0.00          1\n",
      "    label_id: 3                                              0.00       0.00       0.00          1\n",
      "    label_id: 4                                              0.00       0.00       0.00          1\n",
      "    label_id: 5                                             50.00     100.00      66.67          1\n",
      "    label_id: 6                                            100.00     100.00     100.00          1\n",
      "    label_id: 7                                            100.00     100.00     100.00          1\n",
      "    label_id: 8                                             33.33     100.00      50.00          1\n",
      "    -------------------\n",
      "    micro avg                                               44.44      44.44      44.44          9\n",
      "    macro avg                                               31.48      44.44      35.19          9\n",
      "    weighted avg                                            31.48      44.44      35.19          9\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.5892386436462402,\n",
       "  'test_precision': 44.44443893432617,\n",
       "  'test_f1': 44.44443893432617,\n",
       "  'test_recall': 44.44443893432617}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the path of the best checkpoint from the training, you may update it to any checkpoint\n",
    "checkpoint_path = trainer.checkpoint_callback.best_model_path\n",
    "# Create an evaluation model and load the checkpoint\n",
    "eval_model = nemo_nlp.models.TextClassificationModel.load_from_checkpoint(checkpoint_path=checkpoint_path)\n",
    "\n",
    "# create a dataloader config for evaluation, the same data file provided in validation_ds is used here\n",
    "# file_path can get updated with any file\n",
    "eval_config = OmegaConf.create({'file_path': config.model.validation_ds.file_path, 'batch_size': 64, 'shuffle': False, 'num_samples': -1})\n",
    "eval_model.setup_test_data(test_data_config=eval_config)\n",
    "#eval_dataloader = eval_model._create_dataloader_from_config(cfg=eval_config, mode='test')\n",
    "\n",
    "# a new trainer is created to show how to evaluate a checkpoint from an already trained model\n",
    "# create a copy of the trainer config and update it to be used for final evaluation\n",
    "eval_trainer_cfg = config.trainer.copy()\n",
    "eval_trainer_cfg.accelerator = 'gpu' if torch.cuda.is_available() else 'cpu' # it is safer to perform evaluation on single GPU as PT is buggy with the last batch on multi-GPUs\n",
    "eval_trainer_cfg.strategy = None # 'ddp' is buggy with test process in the current PT, it looks like it has been fixed in the latest master\n",
    "eval_trainer = pl.Trainer(**eval_trainer_cfg)\n",
    "eval_trainer.test(model=eval_model, verbose=False) # test_dataloaders=eval_dataloader,"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Text_Classification_Sentiment_Analysis.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
